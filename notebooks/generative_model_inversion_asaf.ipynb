{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "9dNiDq3h5wHD",
    "KMmJ_V1k50x8",
    "a0mvcDjVD7jI",
    "BzWHsBlPHO_m",
    "RMDEFe7iciZQ",
    "rCUAnPGSNe_Y"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9a3329a5bd7b4fa7b8ab1628dbb856e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea6491f4869e4cb29fc4c6dd885816f5",
       "IPY_MODEL_5d933e35471f4c54a1bea9a21bef95d8",
       "IPY_MODEL_4331ca9017a840ce9235ec5299a0ae11"
      ],
      "layout": "IPY_MODEL_5baf550b09b14eed83daec5a81650e50"
     }
    },
    "ea6491f4869e4cb29fc4c6dd885816f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22d2e85e5ada45a492b13ce1283ec570",
      "placeholder": "​",
      "style": "IPY_MODEL_277ef6f2ed534a95b3a8c4852538810d",
      "value": "100%"
     }
    },
    "5d933e35471f4c54a1bea9a21bef95d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_745948d5784948f68b9e566c7458fcdf",
      "max": 111898327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b807b34fbec94bdba8039772d27b878b",
      "value": 111898327
     }
    },
    "4331ca9017a840ce9235ec5299a0ae11": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d23cb0d466f94bf6b60a51324177e23a",
      "placeholder": "​",
      "style": "IPY_MODEL_1c0060f431994dbb8888babd7f57980b",
      "value": " 107M/107M [00:03&lt;00:00, 32.8MB/s]"
     }
    },
    "5baf550b09b14eed83daec5a81650e50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22d2e85e5ada45a492b13ce1283ec570": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "277ef6f2ed534a95b3a8c4852538810d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "745948d5784948f68b9e566c7458fcdf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b807b34fbec94bdba8039772d27b878b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d23cb0d466f94bf6b60a51324177e23a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c0060f431994dbb8888babd7f57980b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "id": "iBHF3CcA2XvT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
   ],
   "metadata": {
    "id": "2eGYoEMQMrZB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e580ccfc-2fd1-4e36-f64a-ecb07275ef06"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install insightface onnxruntime-gpu --quiet"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8DoNqFLY5pY",
    "outputId": "6b49f2e3-24a1-41aa-e2e4-7e866903df22"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install facenet-pytorch ninja lpips torchmetrics --force-reinstall --no-cache-dir"
   ],
   "metadata": {
    "id": "12akwXGaMDAr",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "f2bd3a61-a38c-48b9-c0ec-60b0c6af9109"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")"
   ],
   "metadata": {
    "id": "6ODesfH3Mq7H"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Download the stylegan2-ada-pytorch FFHQ model (resolution 1024x1024)\n",
    "# This is hosted by NVIDIA\n",
    "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O models/ffhq.pkl\n",
    "\n",
    "print(\"Download complete.\")"
   ],
   "metadata": {
    "id": "GEfKn9abNwVv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e507196a-971d-456d-c5de-afedf0f71320"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "import dnnlib\n",
    "import legacy # From the cloned repo\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import lpips\n",
    "import math\n",
    "import random\n",
    "\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis"
   ],
   "metadata": {
    "id": "_auF-N3R6jot"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "metadata": {
    "id": "PX-okYJJMII0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "outputId": "6cece658-87d5-4e13-a827-041198aa2dae"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior (might slow down slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(0)"
   ],
   "metadata": {
    "id": "vjfSB7VDWY82"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# load dictionary of {filename: embedding_vector}\n",
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ],
   "metadata": {
    "id": "zDcj2adn6tfp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class StyleGANGenerator(torch.nn.Module):\n",
    "    def __init__(self, network_pkl):\n",
    "        super(StyleGANGenerator, self).__init__()\n",
    "        print(f'Loading network from \"{network_pkl}\"...')\n",
    "\n",
    "        with dnnlib.util.open_url(network_pkl) as f:\n",
    "            # Load the network from the pickle file\n",
    "            network_dict = legacy.load_network_pkl(f)\n",
    "            self.G = network_dict['G_ema'].to(device)\n",
    "            self.D = network_dict['D'].to(device)\n",
    "            self.D.eval()\n",
    "            for param in self.D.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Lock the weights (we never train the generator itself)\n",
    "        self.G.eval()\n",
    "        for param in self.G.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Store useful constants\n",
    "        self.w_dim = self.G.w_dim  # Usually 512\n",
    "        self.num_ws = self.G.mapping.num_ws # Usually 18 for 1024x1024\n",
    "        print(f'Loaded network! (w_dim: {self.w_dim}, num_ws: {self.num_ws})')\n",
    "\n",
    "    def forward(self, w_plus_vector):\n",
    "        \"\"\"\n",
    "        Input: w_plus_vector of shape (Batch, 18, 512)\n",
    "        Output: Image tensor (Batch, 3, 1024, 1024) in range [-1, 1]\n",
    "        \"\"\"\n",
    "        # synthesis() expects input to be split by layers, but w+ is already shaped correctly\n",
    "        # noise_mode='const' means we don't add random noise to hair/pores every time (deterministic)\n",
    "        img = self.G.synthesis(w_plus_vector, noise_mode='const')\n",
    "        return img\n",
    "\n",
    "    def get_mean_w(self, n_samples=4096, seed=0):\n",
    "        \"\"\"\n",
    "        Get the average latent code (W space).\n",
    "        Optimizing starting from the Mean Face is much faster/easier.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        z = torch.randn(n_samples, self.G.z_dim, device=device)\n",
    "        w = self.G.mapping(z, None) # Convert z to w\n",
    "        w_avg = w.mean(0, keepdim=True)\n",
    "\n",
    "        return w_avg\n",
    "\n",
    "# Initialize the model\n",
    "generator = StyleGANGenerator('models/ffhq.pkl')\n",
    "print(\"Generator Loaded Successfully!\")"
   ],
   "metadata": {
    "id": "d3OeeVceO3r-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7695d7f7-0b3d-48d4-fd6d-8a6bb1608f52"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Get the mean latent code\n",
    "w_mean = generator.get_mean_w()\n",
    "\n",
    "# 2. Generate the image\n",
    "with torch.no_grad():\n",
    "    generated_img_tensor = generator(w_mean)\n",
    "\n",
    "# 3. Convert from [-1, 1] range to [0, 1] for visualization\n",
    "# StyleGAN output is (B, 3, H, W)\n",
    "vis_img = (generated_img_tensor.clamp(-1, 1) + 1) / 2.0\n",
    "vis_img = vis_img[0].cpu() # Take first item in batch\n",
    "\n",
    "# 4. Show it\n",
    "plt.imshow(vis_img.permute(1, 2, 0).numpy())\n",
    "plt.axis('off')\n",
    "plt.title(\"The Average Person (Mean Face)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "bGGgwZvbPpfl",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "outputId": "2cf5685e-71a4-41ec-f3f0-620147041bbf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160,160)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "metadata": {
    "id": "u6s_GrLsTpE-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9a3329a5bd7b4fa7b8ab1628dbb856e6",
      "ea6491f4869e4cb29fc4c6dd885816f5",
      "5d933e35471f4c54a1bea9a21bef95d8",
      "4331ca9017a840ce9235ec5299a0ae11",
      "5baf550b09b14eed83daec5a81650e50",
      "22d2e85e5ada45a492b13ce1283ec570",
      "277ef6f2ed534a95b3a8c4852538810d",
      "745948d5784948f68b9e566c7458fcdf",
      "b807b34fbec94bdba8039772d27b878b",
      "d23cb0d466f94bf6b60a51324177e23a",
      "1c0060f431994dbb8888babd7f57980b"
     ]
    },
    "outputId": "ce47d616-6e44-4e82-c5d3-c889d9d65ffd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00000 = Image.open(\"00000.jpg\").convert(\"RGB\")\n",
    "x_00000 = transform(img_00000).unsqueeze(0).to(device)\n",
    "img_00001 = Image.open(\"00001.jpg\").convert(\"RGB\")\n",
    "x_00001 = transform(img_00001).unsqueeze(0).to(device)\n",
    "img_00002 = Image.open(\"00002.jpg\").convert(\"RGB\")\n",
    "x_00002 = transform(img_00002).unsqueeze(0).to(device)"
   ],
   "metadata": {
    "id": "i84jLAGqTtU1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "emb_00000 = model(x_00000*2-1).detach().cpu()\n",
    "emb_00001 = model(x_00001*2-1).detach().cpu()\n",
    "emb_00002 = model(x_00002*2-1).detach().cpu()"
   ],
   "metadata": {
    "id": "pNtvk64QTv8U"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "target_embedding = emb_00001.to(device)\n",
    "target_image = x_00001"
   ],
   "metadata": {
    "id": "19ZDmahOT9xv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# General Functions"
   ],
   "metadata": {
    "id": "rH8RCLPrGkPD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "\n",
    "        # Load VGG16\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
    "\n",
    "        # Slicing up to layer 16 (ReLU3_3) is standard.\n",
    "        self.blocks = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
    "\n",
    "        # Freeze the model weights\n",
    "        for param in self.blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # VGG specific normalization\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
    "        self.resize = resize\n",
    "\n",
    "    def forward(self, generated_img, target_img):\n",
    "        # Assuming the images are in [0, 1] range:\n",
    "        gen_norm = (generated_img - self.mean) / self.std\n",
    "        target_norm = (target_img - self.mean) / self.std\n",
    "\n",
    "        # Extract features\n",
    "        gen_features = self.blocks(gen_norm)\n",
    "        target_features = self.blocks(target_norm)\n",
    "\n",
    "        # Calculate L2 loss between the feature maps\n",
    "        loss = torch.nn.functional.mse_loss(gen_features, target_features)\n",
    "        return loss\n",
    "\n",
    "perceptual_criterion = VGGPerceptualLoss().to(device)"
   ],
   "metadata": {
    "id": "Y2yMOtXDUuke",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b11365e2-dbb2-4e80-c115-b90ee1621d65"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def display_grid_graphs(metrics_dict, n_cols=2, steps_log=None, log_scale_keys=None, figsize=None):\n",
    "    \"\"\"\n",
    "    Plots multiple graphs in a grid.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary where Key is the Title and Value is the list of data.\n",
    "        n_cols (int): Number of columns in the grid.\n",
    "        steps_log (list): list of step jumps. If None, include all the steps.\n",
    "        log_scale_keys (list): List of keys from metrics_dict that should be plotted in log scale.\n",
    "        figsize (tuple): Optional custom size (width, height). If None, calculates automatically.\n",
    "    \"\"\"\n",
    "    if steps_log is None:\n",
    "        steps_log = list(range(len(next(iter(metrics_dict.values())))))\n",
    "\n",
    "    if log_scale_keys is None:\n",
    "        log_scale_keys = []\n",
    "\n",
    "    # Calculate Grid Dimensions\n",
    "    n = len(metrics_dict)\n",
    "    n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "    # Auto-calculate figure size if not provided\n",
    "    if figsize is None:\n",
    "        figsize = (4 * n_cols, 3 * n_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Plot Data\n",
    "    for i, (label, values) in enumerate(metrics_dict.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(steps_log, values)\n",
    "\n",
    "        ax.set_title(f\"{label} per Step\")\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(label)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if label in log_scale_keys:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "    # Hide empty subplots (if n is not a perfect multiple of n_cols)\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "r2kij7jnGmqB"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def save_and_display_image(image, filename):\n",
    "    image = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    image_pil = transforms.ToPILImage()(image)\n",
    "    image_pil.save(filename)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # display(final_image)"
   ],
   "metadata": {
    "id": "0biMIEDZYIbE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "lpips_metric = lpips.LPIPS(net='vgg').to(device)\n",
    "# < 0.25 high similarity\n",
    "# > 0.7 different images\n",
    "\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "# > 30 dB: High quality (hard to distinguish difference).\n",
    "# 20-30 dB: Acceptable quality.\n",
    "# < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "# 1.0: Identical images.\n",
    "# > 0.9: Very structurally similar."
   ],
   "metadata": {
    "id": "2prbcP_DGu_u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eec9ddae-66b8-449e-883d-e02f6df2b6f3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_and_log(i, iterations, current_img, target_img, history, freq=20, grayscale=False):\n",
    "    \"\"\"\n",
    "    Evaluates metrics and updates history lists in-place.\n",
    "\n",
    "    Args:\n",
    "        i (int): Current iteration.\n",
    "        iterations (int): Total iterations.\n",
    "        current_img (Tensor): The normalized image (output of tanh, [-1, 1]).\n",
    "        target_img (Tensor): The target image ([0, 1]).\n",
    "        history (tuple): (lpips_list, psnr_list, ssim_list, steps).\n",
    "        freq (int): Log frequency.\n",
    "    \"\"\"\n",
    "    if i % freq != 0 and i != iterations - 1:\n",
    "        return\n",
    "\n",
    "    lpips_list, psnr_list, ssim_list, steps = history\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert [-1, 1] -> [0, 1]\n",
    "        val_img = (current_img * 0.5) + 0.5\n",
    "        tgt_img = target_img\n",
    "\n",
    "        # Clamp to ensure numerical stability (fix float errors like -0.0001 or 1.0001)\n",
    "        val_img = val_img.clamp(0, 1)\n",
    "        tgt_img = tgt_img.clamp(0, 1)\n",
    "\n",
    "        if grayscale:\n",
    "            val_img = transforms.functional.rgb_to_grayscale(val_img, num_output_channels=3)\n",
    "            tgt_img = transforms.functional.rgb_to_grayscale(tgt_img, num_output_channels=3)\n",
    "\n",
    "        lpips_list.append(lpips_metric(val_img, tgt_img).item())\n",
    "        psnr_list.append(psnr_metric(val_img, tgt_img).item())\n",
    "        ssim_list.append(ssim_metric(val_img, tgt_img).item())\n",
    "        steps.append(i)"
   ],
   "metadata": {
    "id": "2GLEelKZGx3o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 11"
   ],
   "metadata": {
    "id": "9dNiDq3h5wHD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "PtqOnhdDSTFF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp11_gmi_b\"\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "w_avg = generator.get_mean_w()\n",
    "latent_code = w_avg.clone().detach().to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)"
   ],
   "metadata": {
    "id": "Urkl-ZUNSRr4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "wnEnV1JjSlnO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(latent_code)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    # Penalize if the code gets too far from the average face (prevents \"weird\" artifacts)\n",
    "    loss_reg = torch.mean((latent_code - w_avg) ** 2)\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i, iterations, generated_image_160, target_image, history_lists, freq=20)\n",
    "    # perceptual_input = (generated_image_160 * 0.5) + 0.5\n",
    "    # perceptual_with_real = perceptual_criterion(perceptual_input, target_image)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "    # perceptual_list.append(perceptual_with_real.item())\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "id": "8VMiAaFBSRoW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "id": "SI75uK6hL7HW"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS\": lpips_list,\n",
    "    \"PSNR\": psnr_list,\n",
    "    \"SSIM\": ssim_list\n",
    "}, n_cols=3, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "R9JBuetfMKgu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(latent_code)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "tfafyBArMZ-1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00000"
   ],
   "metadata": {
    "id": "IRzCDV5ySRgJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "hpCR732H1Lu0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "_lpips_list, _psnr_list, _ssim_list, _steps = [], [], [], []\n",
    "_history_lists = (_lpips_list, _psnr_list, _ssim_list, _steps)"
   ],
   "metadata": {
    "id": "YpSTvPKiTcjS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate_and_log(0, 1000, generated_image_160, target_image, _history_lists, freq=20, grayscale=False)\n",
    "evaluate_and_log(0, 1000, generated_image_160, target_image, _history_lists, freq=20, grayscale=True)"
   ],
   "metadata": {
    "id": "c6B8iCaQTBQ-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "_history_lists"
   ],
   "metadata": {
    "id": "dFp3gbXhTp-t"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 12 - decaying reg weight"
   ],
   "metadata": {
    "id": "KMmJ_V1k50x8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "xIxyPD_X50x9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp12_gmi_dacaying_reg_weight\"\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "w_avg = generator.get_mean_w()\n",
    "latent_code = w_avg.clone().detach().to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.005  # reg loss - deviation from average face\n",
    "reg_weight_start = 0.05\n",
    "reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)"
   ],
   "metadata": {
    "id": "BHdzwbMs50x9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "Qb0ZtLqo50x9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(latent_code)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((latent_code - w_avg) ** 2)\n",
    "\n",
    "    current_weight = reg_weight_start + (reg_weight_end - reg_weight_start) * (i / iterations)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (current_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i, iterations, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "id": "O-mDWWLQ50x-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "id": "XeE0ElH950x_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS\": lpips_list,\n",
    "    \"PSNR\": psnr_list,\n",
    "    \"SSIM\": ssim_list\n",
    "}, n_cols=3, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "Uqd47e9_50x_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(latent_code)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "eM0JvD2750x_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00000"
   ],
   "metadata": {
    "id": "GkhhGgxR50x_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 13 - optimizing w instead of w+"
   ],
   "metadata": {
    "id": "a0mvcDjVD7jI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "pIij3z16D7jJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp12_gmi_mahalanobis_dist\"\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "w_avg = generator.get_mean_w()\n",
    "w_single = w_avg[:, 0, :].clone().detach()\n",
    "latent_code = w_single.to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.005  # reg loss - deviation from average face\n",
    "reg_weight_start = 0.05\n",
    "reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)"
   ],
   "metadata": {
    "id": "QkRMmfFcD7jJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "49EeeOjgD7jJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_stack)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "    current_weight = reg_weight_start + (reg_weight_end - reg_weight_start) * (i / iterations)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (current_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i, iterations, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "id": "al9kBFN8D7jK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "id": "emTbOSajD7jK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS\": lpips_list,\n",
    "    \"PSNR\": psnr_list,\n",
    "    \"SSIM\": ssim_list\n",
    "}, n_cols=3, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "QhCgkZmHD7jK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_stack)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "BD2Qc_EHD7jK"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "id": "A31HDbDeD7jL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 14 - coarse to fine - w then w+"
   ],
   "metadata": {
    "id": "BzWHsBlPHO_m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "7W1zDuO_HO_n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp14_gmi_coarse_to_fine\"\n",
    "\n",
    "iterations_1 = 25\n",
    "\n",
    "w_avg = generator.get_mean_w(seed=15)\n",
    "w_single = w_avg[:, 0, :].clone().detach()\n",
    "latent_code = w_single.to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations_1)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "# reg_weight_start = 0.05\n",
    "# reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
    "\n",
    "images = []"
   ],
   "metadata": {
    "id": "BOtg9VM_HO_o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "uCIgmfe0HO_o"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations_1)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_stack)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i, iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_1}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "iterations_2 = 350\n",
    "w_stack = latent_code.detach().unsqueeze(1).repeat(1, 18, 1)\n",
    "w_plus = w_stack.clone()\n",
    "w_plus.requires_grad = True\n",
    "optimizer_w_plus = optim.Adam([w_plus], lr=0.025)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=iterations_2)\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "\n",
    "for i in tqdm(range(iterations_2)):\n",
    "    optimizer_w_plus.zero_grad()\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_plus)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((w_plus - w_avg) ** 2)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i + iterations_1, iterations_2 + iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer_w_plus.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_2}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "id": "PnszmCloHO_o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "id": "JgyE3YMxHO_p"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS\": lpips_list,\n",
    "    \"PSNR\": psnr_list,\n",
    "    \"SSIM\": ssim_list\n",
    "}, n_cols=3, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "U_ktWbG8HO_p"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_stack)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "-4e5Tbt7KzyE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_plus)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "cAUABm44HO_q"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "id": "v7Tgb6rRHO_q"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for img in images:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "D4n9CCzpTrCO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "uqaqjfa3Y__U"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 15 - find best seed"
   ],
   "metadata": {
    "id": "RMDEFe7iciZQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "63oMfbfAciZR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp15_gmi_best_seed\"\n",
    "\n",
    "iterations_1 = 25\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "# reg_weight_start = 0.05\n",
    "# reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
    "\n",
    "images = []"
   ],
   "metadata": {
    "id": "E7fNoXEeciZR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "K7RICBJ2ciZR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "w_stack_list = []\n",
    "for seed in range(10, 25):\n",
    "    print(f\"Seed: {seed}\")\n",
    "    w_avg = generator.get_mean_w(seed=seed)\n",
    "    w_single = w_avg[:, 0, :].clone().detach()\n",
    "    latent_code = w_single.to(device)\n",
    "    latent_code.requires_grad = True\n",
    "\n",
    "    optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations_1)\n",
    "\n",
    "    for i in tqdm(range(iterations_1)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "        # Generate the image\n",
    "        generated_image_1024 = generator(w_stack)\n",
    "\n",
    "        # Resize for FaceNet (160x160)\n",
    "        generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Get Embedding\n",
    "        current_embedding = model(generated_image_160)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "        loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "        cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "        evaluate_and_log(i, iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if i == 0 or (i + 1) % 100 == 0:\n",
    "            print(f\"Step [{i+1}/{iterations_1}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "            img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "            images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "        loss_list.append(total_loss.item())\n",
    "        cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "    w_stack_list.append((seed, w_stack))"
   ],
   "metadata": {
    "id": "w7Ph500qciZR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    for seed, w_stack in w_stack_list:\n",
    "        final_high_res = generator(w_stack)\n",
    "        final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "        print(f\"Seed: {seed}\")\n",
    "        save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "pK18oUaMciZS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "id": "JrmM9LX-dbU1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FMwtVXWVeEwx"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 16 - adding discriminator loss"
   ],
   "metadata": {
    "id": "rCUAnPGSNe_Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "ajEJpv-4Ne_Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp16_gmi_discriminator_loss\"\n",
    "\n",
    "iterations_1 = 25\n",
    "\n",
    "w_avg = generator.get_mean_w(seed=15)\n",
    "w_single = w_avg[:, 0, :].clone().detach()\n",
    "latent_code = w_single.to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations_1)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "# reg_weight_start = 0.05\n",
    "# reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
    "\n",
    "images = []"
   ],
   "metadata": {
    "id": "P_D1evSJNe_a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "XPEyNYmtNe_a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations_1)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_stack)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i, iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_1}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "iterations_2 = 350\n",
    "w_stack = latent_code.detach().unsqueeze(1).repeat(1, 18, 1)\n",
    "w_plus = w_stack.clone()\n",
    "w_plus.requires_grad = True\n",
    "optimizer_w_plus = optim.Adam([w_plus], lr=0.025)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=iterations_2)\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "d_loss_weight = 0.0001\n",
    "\n",
    "for i in tqdm(range(iterations_2)):\n",
    "    optimizer_w_plus.zero_grad()\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_plus)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((w_plus - w_avg) ** 2)\n",
    "    d_logits = generator.D(generated_image_1024, c=None)\n",
    "    loss_discriminator = torch.nn.functional.softplus(-d_logits).mean()\n",
    "    # print(f\"loss_mse: {loss_mse}. loss_reg: {loss_reg}. loss_discriminator: {loss_discriminator}\")\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg) + (d_loss_weight * loss_discriminator)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log(i + iterations_1, iterations_2 + iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer_w_plus.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_2}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_1024.detach().clamp(-1, 1) + 1) / 2.0\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "id": "bAHKgKwONe_b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "id": "qtFtOnT6Ne_c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS\": lpips_list,\n",
    "    \"PSNR\": psnr_list,\n",
    "    \"SSIM\": ssim_list\n",
    "}, n_cols=3, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\"\"\")"
   ],
   "metadata": {
    "id": "16C0DTCaNe_c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_stack)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "O92UgKJTNe_d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_plus)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "id": "UUl1beA6Ne_d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "id": "mXYoUs7dNe_d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for img in images:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "t8GueyhMNe_e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2qfW-caFNe_e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 17 - adding arcface metric"
   ],
   "metadata": {
    "id": "waJmVSZXZptE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ArcFaceMetric:\n",
    "    def __init__(self, device='cuda'):\n",
    "        # Load the default 'buffalo_l' model pack (contains ResNet50 ArcFace)\n",
    "        self.app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider'])\n",
    "        self.app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "        self.handler = self.app.models['recognition'] # The ArcFace recognition model\n",
    "        self.device = device\n",
    "\n",
    "    def get_embedding(self, tensor_img):\n",
    "        \"\"\"\n",
    "        Input: Tensor [1, 3, H, W] in range [-1, 1] or [0, 1], RGB\n",
    "        Output: Numpy Array [512]\n",
    "        \"\"\"\n",
    "        # 1. Convert Tensor to Numpy Image [H, W, 3] in range [0, 255]\n",
    "        # Assuming input is [-1, 1] (tanh output)\n",
    "        if tensor_img.min() < 0:\n",
    "            img = (tensor_img * 0.5 + 0.5)\n",
    "        else:\n",
    "            img = tensor_img\n",
    "\n",
    "        img = img.clamp(0, 1).cpu().detach().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        # 2. Convert RGB to BGR (InsightFace expects BGR via OpenCV)\n",
    "        img_bgr = img[:, :, ::-1]\n",
    "\n",
    "        # 3. Resize to 112x112 (ArcFace standard input)\n",
    "        # Note: Usually we use alignment (warping), but for simple metric\n",
    "        # on already cropped faces, resizing is acceptable.\n",
    "        import cv2\n",
    "        img_resized = cv2.resize(img_bgr, (112, 112))\n",
    "\n",
    "        # 4. Get Embedding (blob is a formatting helper)\n",
    "        blob = cv2.dnn.blobFromImage(img_resized, 1.0 / 127.5, (112, 112), (127.5, 127.5, 127.5), swapRB=True)\n",
    "        # Note: InsightFace handler expects raw forward pass usually,\n",
    "        # but calling .get_feat is safer if wrapping 'get'\n",
    "        # We can simulate the forward pass directly on the handler:\n",
    "        embedding = self.handler.get_feat(img_resized)\n",
    "\n",
    "        return embedding.flatten()\n",
    "\n",
    "    def compute_sim(self, tensor_gen, tensor_target):\n",
    "        emb_gen = self.get_embedding(tensor_gen)\n",
    "        emb_target = self.get_embedding(tensor_target)\n",
    "\n",
    "        # Compute Cosine Similarity\n",
    "        from numpy.linalg import norm\n",
    "        sim = np.dot(emb_gen, emb_target) / (norm(emb_gen) * norm(emb_target))\n",
    "        return sim\n",
    "\n",
    "# Initialize ONCE (it takes time to load)\n",
    "arcface_metric = ArcFaceMetric(device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UfxU5v3MKQ77",
    "outputId": "c74fc89d-1d9f-45a4-a2f1-a7786b4bceaa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_and_log2(i, iterations, current_img, target_img, history, freq=20, grayscale=False):\n",
    "    \"\"\"\n",
    "    Evaluates metrics and updates history lists in-place.\n",
    "\n",
    "    Args:\n",
    "        i (int): Current iteration.\n",
    "        iterations (int): Total iterations.\n",
    "        current_img (Tensor): The normalized image (output of tanh, [-1, 1]).\n",
    "        target_img (Tensor): The target image ([0, 1]).\n",
    "        history (tuple): (lpips_list, psnr_list, ssim_list, steps).\n",
    "        freq (int): Log frequency.\n",
    "    \"\"\"\n",
    "    if i % freq != 0 and i != iterations - 1:\n",
    "        return\n",
    "\n",
    "    lpips_list, psnr_list, ssim_list, id_score_list, steps = history\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert [-1, 1] -> [0, 1]\n",
    "        val_img = (current_img * 0.5) + 0.5\n",
    "        tgt_img = target_img\n",
    "\n",
    "        # Clamp to ensure numerical stability (fix float errors like -0.0001 or 1.0001)\n",
    "        val_img = val_img.clamp(0, 1)\n",
    "        tgt_img = tgt_img.clamp(0, 1)\n",
    "\n",
    "        if grayscale:\n",
    "            val_img = transforms.functional.rgb_to_grayscale(val_img, num_output_channels=3)\n",
    "            tgt_img = transforms.functional.rgb_to_grayscale(tgt_img, num_output_channels=3)\n",
    "\n",
    "        lpips_list.append(lpips_metric(val_img, tgt_img).item())\n",
    "        psnr_list.append(psnr_metric(val_img, tgt_img).item())\n",
    "        ssim_list.append(ssim_metric(val_img, tgt_img).item())\n",
    "\n",
    "        arcface_val_img = val_img * 2 - 1\n",
    "        arcface_tgt_img = tgt_img * 2 - 1\n",
    "        id_sim = arcface_metric.compute_sim(arcface_val_img, arcface_tgt_img)\n",
    "        id_score_list.append(id_sim)\n",
    "\n",
    "        steps.append(i)"
   ],
   "metadata": {
    "id": "bGdgkbfTaSef"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "zFPznGS8ZptE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp17_gmi_coarse_to_fine\"\n",
    "\n",
    "iterations_1 = 25\n",
    "\n",
    "w_avg = generator.get_mean_w(seed=15)\n",
    "w_single = w_avg[:, 0, :].clone().detach()\n",
    "latent_code = w_single.to(device)\n",
    "latent_code.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations_1)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "# reg_weight_start = 0.05\n",
    "# reg_weight_end = 0.0\n",
    "\n",
    "loss_list = []\n",
    "cosine_similarity_list = []\n",
    "lpips_list = []\n",
    "psnr_list = []\n",
    "ssim_list = []\n",
    "id_score_list = []\n",
    "steps = []\n",
    "history_lists = (lpips_list, psnr_list, ssim_list, id_score_list, steps)\n",
    "\n",
    "images = []"
   ],
   "metadata": {
    "id": "IoImpd_BZptF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "_yNpYxWfZptF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for i in tqdm(range(iterations_1)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_stack)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log2(i, iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_1}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "iterations_2 = 350\n",
    "w_stack = latent_code.detach().unsqueeze(1).repeat(1, 18, 1)\n",
    "w_plus = w_stack.clone()\n",
    "w_plus.requires_grad = True\n",
    "optimizer_w_plus = optim.Adam([w_plus], lr=0.025)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=iterations_2)\n",
    "reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "\n",
    "for i in tqdm(range(iterations_2)):\n",
    "    optimizer_w_plus.zero_grad()\n",
    "\n",
    "    # Generate the image\n",
    "    generated_image_1024 = generator(w_plus)\n",
    "\n",
    "    # Resize for FaceNet (160x160)\n",
    "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Get Embedding\n",
    "    current_embedding = model(generated_image_160)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    loss_reg = torch.mean((w_plus - w_avg) ** 2)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    evaluate_and_log2(i + iterations_1, iterations_2 + iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer_w_plus.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i == 0 or (i + 1) % 100 == 0:\n",
    "        print(f\"Step [{i+1}/{iterations_2}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "        img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "        images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    loss_list.append(total_loss.item())\n",
    "    cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "print(\"Inversion Complete.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5a01a826-5cdd-49a8-cb84-b862ed69c32d",
    "id": "XUhFyI3NZptF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"Loss\": loss_list,\n",
    "    \"Cosine Similarity\": cosine_similarity_list\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "outputId": "2865a0c3-9c37-451b-fe4e-0db7d76b41ce",
    "id": "kkAr0iDqZptG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs({\n",
    "    \"LPIPS↓\": lpips_list,\n",
    "    \"PSNR↑\": psnr_list,\n",
    "    \"SSIM↑\": ssim_list,\n",
    "    \"ArcFace↑\": id_score_list\n",
    "}, n_cols=2, steps_log=steps)\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\n",
    "    ArcFace:\n",
    "        > 0.60: Excellent. It is definitely the same person.\n",
    "        0.40 – 0.60: Good. It looks like a \"sibling\" or the same person in very different lighting.\n",
    "        < 0.40: Failure. The optimizer fooled FaceNet, but ArcFace is not convinced.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "outputId": "aa33b5a0-c2cb-487e-fc80-40b892a73b38",
    "id": "Mmsz32hIZptG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_stack)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "outputId": "4a80c7fa-20a3-4e49-980a-6bf0a99095c7",
    "id": "VRBOtdxEZptG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_plus)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "outputId": "bd6e4a35-a84a-4702-f58c-a1a2565ef339",
    "id": "d6eYOif4ZptH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "outputId": "ae6da5fe-1cf8-4b1a-cfd2-e029db80a369",
    "id": "gmtl2b-EZptH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# exp 18 - center of several seeds"
   ],
   "metadata": {
    "id": "9pIG8r43Q6Bg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_embedding = emb_00001.to(device)\n",
    "target_image = x_00001"
   ],
   "metadata": {
    "id": "5FPzXsWYStid"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "4iA98BDpSwM4",
    "outputId": "69e98f2c-2eae-4ee9-83f2-ec6859e43372"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# @title\n",
    "def display_grid_graphs2(metrics_dict, n_cols=2, steps_log=None, log_scale_keys=None, figsize=None):\n",
    "    \"\"\"\n",
    "    Plots multiple graphs in a grid. Supports multiple attempts per metric.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dict):\n",
    "            Key = Title.\n",
    "            Value = List of lists (e.g., [[attempt1_data], [attempt2_data]])\n",
    "                    OR single list (backward compatible).\n",
    "        n_cols (int): Number of columns in the grid.\n",
    "        steps_log (list): X-axis values. If None, auto-generated from data length.\n",
    "        log_scale_keys (list): Keys to plot in log scale.\n",
    "        figsize (tuple): Custom size.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Handle defaults\n",
    "    if log_scale_keys is None:\n",
    "        log_scale_keys = []\n",
    "\n",
    "    # 2. Determine steps_log (X-axis) if not provided\n",
    "    if steps_log is None:\n",
    "        # Peek at the first item to determine length\n",
    "        first_val = next(iter(metrics_dict.values()))\n",
    "        if isinstance(first_val[0], list):\n",
    "            # It's a list of lists, take length of first attempt\n",
    "            data_len = len(first_val[0])\n",
    "        else:\n",
    "            # It's a flat list\n",
    "            data_len = len(first_val)\n",
    "        steps_log = list(range(data_len))\n",
    "\n",
    "    # 3. Calculate Grid Dimensions\n",
    "    n = len(metrics_dict)\n",
    "    n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (5 * n_cols, 4 * n_rows) # Slightly larger default for clarity\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # 4. Plot Data\n",
    "    for i, (label, data) in enumerate(metrics_dict.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Check if data is \"List of Lists\" (Multiple Attempts) or \"List\" (Single Run)\n",
    "        if isinstance(data[0], list):\n",
    "            # --- Multiple Attempts Logic ---\n",
    "            for attempt_idx, attempt_values in enumerate(data):\n",
    "                # Matplotlib cycles colors automatically for each plot call\n",
    "                ax.plot(steps_log, attempt_values, label=f\"Attempt {attempt_idx + 1}\")\n",
    "\n",
    "            # Add legend to distinguish attempts\n",
    "            # ax.legend(fontsize='small')\n",
    "\n",
    "        else:\n",
    "            # --- Single Run Logic (Old compatibility) ---\n",
    "            ax.plot(steps_log, data)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_title(f\"{label}\") # Removed \"per Step\" to keep it clean\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(label)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if label in log_scale_keys:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "    # 5. Hide empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "NShBEsDLT3nQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "id": "6GHiWYOxQ6Bi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_name = \"exp18_several_seeds_center\"\n",
    "\n",
    "def run_attack(seed: int):\n",
    "    print(f\"Currently testing seed: {seed}\")\n",
    "    # Setup --------------------------------------------------------------------\n",
    "    iterations_1 = 25\n",
    "\n",
    "    w_avg = generator.get_mean_w(seed=seed)\n",
    "    w_single = w_avg[:, 0, :].clone().detach()\n",
    "    latent_code = w_single.to(device)\n",
    "    latent_code.requires_grad = True\n",
    "\n",
    "    optimizer = optim.Adam([latent_code], lr=0.05)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations_1)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "    reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "    # reg_weight_start = 0.05\n",
    "    # reg_weight_end = 0.0\n",
    "\n",
    "    loss_list = []\n",
    "    cosine_similarity_list = []\n",
    "    lpips_list = []\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    id_score_list = []\n",
    "    steps = []\n",
    "    history_lists = (lpips_list, psnr_list, ssim_list, id_score_list, steps)\n",
    "\n",
    "    images = []\n",
    "\n",
    "    # Attack Loop --------------------------------------------------------------\n",
    "    for i in tqdm(range(iterations_1)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        w_stack = latent_code.unsqueeze(1).repeat(1, 18, 1)\n",
    "\n",
    "        # Generate the image\n",
    "        generated_image_1024 = generator(w_stack)\n",
    "\n",
    "        # Resize for FaceNet (160x160)\n",
    "        generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Get Embedding\n",
    "        current_embedding = model(generated_image_160)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "        loss_reg = torch.mean((latent_code - w_single.to(device)) ** 2)\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "        cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "        evaluate_and_log2(i, iterations_1, generated_image_160, target_image, history_lists, freq=1)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if i == 0 or (i + 1) % 100 == 0:\n",
    "            print(f\"Step [{i+1}/{iterations_1}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "            img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "            images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "        loss_list.append(total_loss.item())\n",
    "        cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "    iterations_2 = 350\n",
    "    w_stack = latent_code.detach().unsqueeze(1).repeat(1, 18, 1)\n",
    "    # w_plus = w_stack.clone()\n",
    "    # w_plus.requires_grad = True\n",
    "    # optimizer_w_plus = optim.Adam([w_plus], lr=0.025)\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.25, total_iters=iterations_2)\n",
    "    # reg_loss_weight = 0.002  # reg loss - deviation from average face\n",
    "\n",
    "    # for i in tqdm(range(iterations_2)):\n",
    "    #     optimizer_w_plus.zero_grad()\n",
    "\n",
    "    #     # Generate the image\n",
    "    #     generated_image_1024 = generator(w_plus)\n",
    "\n",
    "    #     # Resize for FaceNet (160x160)\n",
    "    #     generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "    #     # Get Embedding\n",
    "    #     current_embedding = model(generated_image_160)\n",
    "\n",
    "    #     # Calculate loss\n",
    "    #     loss_mse = mse_loss(current_embedding, target_embedding)\n",
    "    #     loss_reg = torch.mean((w_plus - w_avg) ** 2)\n",
    "\n",
    "    #     # Total Loss\n",
    "    #     total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
    "\n",
    "    #     cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
    "    #     evaluate_and_log2(i + iterations_1, iterations_2 + iterations_1, generated_image_160, target_image, history_lists, freq=20)\n",
    "\n",
    "    #     total_loss.backward()\n",
    "    #     optimizer_w_plus.step()\n",
    "    #     scheduler.step()\n",
    "\n",
    "    #     if i == 0 or (i + 1) % 100 == 0:\n",
    "    #         print(f\"Step [{i+1}/{iterations_2}], Loss: {total_loss.item():.6f}\")#, Perc: {perceptual_with_real.item():.6f}\")\n",
    "    #         img = (generated_image_160.detach() * 0.5) + 0.5\n",
    "    #         images.append(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    #     loss_list.append(total_loss.item())\n",
    "    #     cosine_similarity_list.append(cos_sim)\n",
    "\n",
    "    final_image = generated_image_160.detach().cpu().squeeze(0)\n",
    "    final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
    "    print(\"Inversion Complete.\")\n",
    "\n",
    "    return loss_list, cosine_similarity_list, lpips_list, psnr_list, ssim_list, id_score_list, steps, w_stack, None#, w_plus"
   ],
   "metadata": {
    "id": "uVWHX86eQ6Bi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "seed_info = []\n",
    "for seed in range(100):\n",
    "    loss_list, cosine_similarity_list, lpips_list, psnr_list, ssim_list, id_score_list, steps, w_stack, w_plus = run_attack(seed)\n",
    "    seed_info.append((seed, loss_list, cosine_similarity_list, lpips_list, psnr_list, ssim_list, id_score_list, steps, w_stack, w_plus))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOTFyufbSHeT",
    "outputId": "665cd289-637f-4b6b-d48c-447b38f7ed1a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attack loop"
   ],
   "metadata": {
    "id": "gCtxYWZ6Q6Bi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs2({\n",
    "    \"Loss\": [info[1] for info in seed_info],\n",
    "    \"Cosine Similarity\": [info[2] for info in seed_info]\n",
    "}, n_cols=3)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "outputId": "d8b75e9d-6ee4-4346-fc74-ab8e2f562ed7",
    "id": "hHLKoQVnQ6Bj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs2({\n",
    "    \"LPIPS↓\": [info[3] for info in seed_info],\n",
    "    \"PSNR↑\": [info[4] for info in seed_info],\n",
    "    \"SSIM↑\": [info[5] for info in seed_info],\n",
    "    \"ArcFace↑\": [info[6] for info in seed_info]\n",
    "}, n_cols=2, steps_log=seed_info[0][7])\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\n",
    "    ArcFace:\n",
    "        > 0.60: Excellent. It is definitely the same person.\n",
    "        0.40 – 0.60: Good. It looks like a \"sibling\" or the same person in very different lighting.\n",
    "        < 0.40: Failure. The optimizer fooled FaceNet, but ArcFace is not convinced.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "aa237856-1155-4a81-a829-8f653fd157b4",
    "id": "lhvo-Ps7Q6Bj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    for info in seed_info:\n",
    "        print(f\"seed: {info[0]}\")\n",
    "        final_high_res = generator(info[8])\n",
    "        final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "        save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "52de3760-fbbf-4675-de4c-c8b2076cfb25",
    "id": "Xg1RqrDrQ6Bj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "img_00001"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "outputId": "ae54ee80-fb04-44dc-f4ad-cb22be392a6f",
    "id": "GX1s0wTdQ6Bj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w_stack_all_attempts = torch.stack([info[8] for info in seed_info], dim=0)\n",
    "w_stack_mean = torch.mean(w_stack_all_attempts, dim=0)\n",
    "w_stack_mean"
   ],
   "metadata": {
    "id": "-QQOPR7EZObH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a8bc5cff-4c0b-4510-ff30-de2ab628a44c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    final_high_res = generator(w_stack_mean)\n",
    "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "    save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "0ne8azIRcALL",
    "outputId": "05a850fa-8276-402d-ba39-3d0055669b74"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "combined_data = torch.cat([w_stack_all_attempts, w_stack_mean.unsqueeze(0)], dim=0)\n",
    "combined_data = combined_data[:, :, 0, :]\n",
    "combined_data = combined_data.view(combined_data.shape[0], -1)\n",
    "combined_data.size()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYRUuWtyc1TX",
    "outputId": "abf54632-29df-45e4-ed51-1c6972929bd5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "n_samples = combined_data.shape[0]\n",
    "perp = min(30, n_samples - 1) if n_samples > 1 else 1"
   ],
   "metadata": {
    "id": "9-m3IKmbdm6K"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE"
   ],
   "metadata": {
    "id": "JaVRQGE5eUm7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tsne = TSNE(n_components=2, perplexity=perp, random_state=42, init='pca', learning_rate='auto')\n",
    "projections = tsne.fit_transform(combined_data.cpu().numpy())\n",
    "\n",
    "# 5. Split back apart\n",
    "points_2d = projections[:-1] # All the regular items\n",
    "mean_2d = projections[-1]    # The last item (The Average)\n",
    "\n",
    "# 6. Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot all sample points\n",
    "plt.scatter(points_2d[:, 0], points_2d[:, 1], c='blue', alpha=0.6, label='Samples')\n",
    "\n",
    "# Plot the Average (Make it distinct)\n",
    "plt.scatter(mean_2d[0], mean_2d[1], c='red', s=200, marker='X', label='High-Dim Average')\n",
    "\n",
    "for i in range(n_samples - 1):\n",
    "    plt.annotate(i, (points_2d[i, 0], points_2d[i, 1]), fontsize=14)\n",
    "\n",
    "plt.title(\"Latent Space Distribution (t-SNE)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "uPi1MsR7eXAt",
    "outputId": "9c762d83-502c-4abe-a059-0b9e64fa909c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "_lpips_list = []\n",
    "_psnr_list = []\n",
    "_ssim_list = []\n",
    "_id_score_list = []\n",
    "_steps = []\n",
    "_history_lists = (_lpips_list, _psnr_list, _ssim_list, _id_score_list, _steps)\n",
    "\n",
    "generated_image_160 = F.interpolate(final_high_res, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "\n",
    "evaluate_and_log2(0, 1, generated_image_160, target_image, _history_lists, freq=1)"
   ],
   "metadata": {
    "id": "7KtouFmweh-S"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "display_grid_graphs2({\n",
    "    \"LPIPS↓\": [info[3] for info in seed_info] + [[_history_lists[0]] * len(seed_info[0][7])],\n",
    "    \"PSNR↑\": [info[4] for info in seed_info] + [[_history_lists[1]] * len(seed_info[0][7])],\n",
    "    \"SSIM↑\": [info[5] for info in seed_info] + [[_history_lists[2]] * len(seed_info[0][7])],\n",
    "    \"ArcFace↑\": [info[6] for info in seed_info] + [[_history_lists[3]] * len(seed_info[0][7])]\n",
    "}, n_cols=2, steps_log=seed_info[0][7])\n",
    "print(\"\"\"\n",
    "    lpips_metric:\n",
    "        < 0.25 high similarity\n",
    "        > 0.7 different images\n",
    "\n",
    "    psnr_metric:\n",
    "        > 30 dB: High quality (hard to distinguish difference).\n",
    "        20-30 dB: Acceptable quality.\n",
    "        < 20 dB: Poor quality (very noisy).\n",
    "\n",
    "    ssim_metric:\n",
    "        1.0: Identical images.\n",
    "        > 0.9: Very structurally similar.\n",
    "\n",
    "    ArcFace:\n",
    "        > 0.60: Excellent. It is definitely the same person.\n",
    "        0.40 – 0.60: Good. It looks like a \"sibling\" or the same person in very different lighting.\n",
    "        < 0.40: Failure. The optimizer fooled FaceNet, but ArcFace is not convinced.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "165c8966-c49a-4210-c44a-2455d4c6a1b7",
    "id": "xeYREmIDiDsv"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    for info in seed_info:\n",
    "        if info[0] not in [62, 80, 1, 76, 26, 36, 10]:\n",
    "            continue\n",
    "        print(f\"seed: {info[0]}\")\n",
    "        final_high_res = generator(info[8])\n",
    "        final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
    "        save_and_display_image(final_img, f\"{experiment_name}.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v7_IhG0Hs9dL",
    "outputId": "2986bc6f-6ceb-4505-9e19-9c7903581756"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# now we need to try to continue optimize the mean w_stack\n",
    "# and find the closest one to him..."
   ],
   "metadata": {
    "id": "xdSHxtnLg9l5"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
