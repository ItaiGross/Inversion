{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iBHF3CcA2XvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "id": "2eGYoEMQMrZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch --force-reinstall --no-cache-dir ninja"
      ],
      "metadata": {
        "id": "12akwXGaMDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")"
      ],
      "metadata": {
        "id": "6ODesfH3Mq7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a folder for models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Download the stylegan2-ada-pytorch FFHQ model (resolution 1024x1024)\n",
        "# This is hosted by NVIDIA\n",
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -O models/ffhq.pkl\n",
        "\n",
        "print(\"Download complete.\")"
      ],
      "metadata": {
        "id": "GEfKn9abNwVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import copy\n",
        "import dnnlib\n",
        "import legacy # From the cloned repo\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_auF-N3R6jot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "PX-okYJJMII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dictionary of {filename: embedding_vector}\n",
        "with open(\"embeddings.pkl\", \"rb\") as f:\n",
        "    embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "zDcj2adn6tfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleGANGenerator(torch.nn.Module):\n",
        "    def __init__(self, network_pkl):\n",
        "        super(StyleGANGenerator, self).__init__()\n",
        "        print(f'Loading network from \"{network_pkl}\"...')\n",
        "\n",
        "        with dnnlib.util.open_url(network_pkl) as f:\n",
        "            # Load the network from the pickle file\n",
        "            self.G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
        "\n",
        "        # Lock the weights (we never train the generator itself)\n",
        "        self.G.eval()\n",
        "        for param in self.G.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Store useful constants\n",
        "        self.w_dim = self.G.w_dim  # Usually 512\n",
        "        self.num_ws = self.G.mapping.num_ws # Usually 18 for 1024x1024\n",
        "        print(f'Loaded network! (w_dim: {self.w_dim}, num_ws: {self.num_ws})')\n",
        "\n",
        "    def forward(self, w_plus_vector):\n",
        "        \"\"\"\n",
        "        Input: w_plus_vector of shape (Batch, 18, 512)\n",
        "        Output: Image tensor (Batch, 3, 1024, 1024) in range [-1, 1]\n",
        "        \"\"\"\n",
        "        # synthesis() expects input to be split by layers, but w+ is already shaped correctly\n",
        "        # noise_mode='const' means we don't add random noise to hair/pores every time (deterministic)\n",
        "        img = self.G.synthesis(w_plus_vector, noise_mode='const')\n",
        "        return img\n",
        "\n",
        "    def get_mean_w(self, n_samples=4096):\n",
        "        \"\"\"\n",
        "        Get the average latent code (W space).\n",
        "        Optimizing starting from the Mean Face is much faster/easier.\n",
        "        \"\"\"\n",
        "        z = torch.randn(n_samples, self.G.z_dim, device=device)\n",
        "        w = self.G.mapping(z, None) # Convert z to w\n",
        "        w_avg = w.mean(0, keepdim=True)\n",
        "\n",
        "        return w_avg\n",
        "\n",
        "# Initialize the model\n",
        "generator = StyleGANGenerator('models/ffhq.pkl')\n",
        "print(\"Generator Loaded Successfully!\")"
      ],
      "metadata": {
        "id": "d3OeeVceO3r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the mean latent code\n",
        "w_mean = generator.get_mean_w()\n",
        "\n",
        "# 2. Generate the image\n",
        "with torch.no_grad():\n",
        "    generated_img_tensor = generator(w_mean)\n",
        "\n",
        "# 3. Convert from [-1, 1] range to [0, 1] for visualization\n",
        "# StyleGAN output is (B, 3, H, W)\n",
        "vis_img = (generated_img_tensor.clamp(-1, 1) + 1) / 2.0\n",
        "vis_img = vis_img[0].cpu() # Take first item in batch\n",
        "\n",
        "# 4. Show it\n",
        "plt.imshow(vis_img.permute(1, 2, 0).numpy())\n",
        "plt.axis('off')\n",
        "plt.title(\"The Average Person (Mean Face)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bGGgwZvbPpfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160,160)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "u6s_GrLsTpE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001 = Image.open(\"00001.jpg\").convert(\"RGB\")\n",
        "x_00001 = transform(img_00001).unsqueeze(0).to(device)\n",
        "img_00002 = Image.open(\"00002.jpg\").convert(\"RGB\")\n",
        "x_00002 = transform(img_00002).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "i84jLAGqTtU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_00001 = model(x_00001*2-1).detach().cpu().numpy()[0]\n",
        "emb_00002 = model(x_00002*2-1).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "pNtvk64QTv8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = torch.tensor(emb_00001, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "target_image = x_00001"
      ],
      "metadata": {
        "id": "19ZDmahOT9xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "\n",
        "        # Load VGG16\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Slicing up to layer 16 (ReLU3_3) is standard.\n",
        "        self.blocks = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
        "\n",
        "        # Freeze the model weights\n",
        "        for param in self.blocks.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # VGG specific normalization\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
        "        self.resize = resize\n",
        "\n",
        "    def forward(self, generated_img, target_img):\n",
        "        # Assuming the images are in [0, 1] range:\n",
        "        gen_norm = (generated_img - self.mean) / self.std\n",
        "        target_norm = (target_img - self.mean) / self.std\n",
        "\n",
        "        # Extract features\n",
        "        gen_features = self.blocks(gen_norm)\n",
        "        target_features = self.blocks(target_norm)\n",
        "\n",
        "        # Calculate L2 loss between the feature maps\n",
        "        loss = torch.nn.functional.mse_loss(gen_features, target_features)\n",
        "        return loss\n",
        "\n",
        "perceptual_criterion = VGGPerceptualLoss().to(device)"
      ],
      "metadata": {
        "id": "Y2yMOtXDUuke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_loss_graph(loss_list, log_scale=False):\n",
        "    plt.plot(loss_list)\n",
        "    if log_scale:\n",
        "        plt.yscale('log')\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss per Step\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "JISqrkThYIbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_image(image, filename):\n",
        "    final_image = torch.tanh(image.detach().cpu().squeeze(0))\n",
        "    final_image = (final_image * 0.5) + 0.5\n",
        "\n",
        "    final_image = transforms.ToPILImage()(final_image)\n",
        "    final_image.save(filename)\n",
        "    display(final_image)"
      ],
      "metadata": {
        "id": "0biMIEDZYIbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "PtqOnhdDSTFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 1000\n",
        "\n",
        "# A. Initialize the Latent Code (The \"Input\" we optimize)\n",
        "# We start with the Mean W because it's the most stable starting point.\n",
        "w_avg = generator.get_mean_w() # Shape: (1, 18, 512)\n",
        "\n",
        "# Make a copy that requires gradients\n",
        "latent_code = w_avg.clone().detach().to(device)\n",
        "latent_code.requires_grad = True\n",
        "\n",
        "# B. The Optimizer\n",
        "# We optimize the latent code, NOT the image.\n",
        "# Note: Learning rate for W space is usually higher (0.01 to 0.1) than pixel optimization.\n",
        "optimizer = optim.Adam([latent_code], lr=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=iterations)\n",
        "\n",
        "# C. Define Losses\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "cosine_loss = torch.nn.CosineEmbeddingLoss()\n",
        "# Note: For Cosine loss, we need a target label '1' (meaning \"similar\")\n",
        "target_label = torch.tensor([1]).to(device)\n",
        "\n",
        "reg_loss_weight = 0.001"
      ],
      "metadata": {
        "id": "Urkl-ZUNSRr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attack loop"
      ],
      "metadata": {
        "id": "wnEnV1JjSlnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "perceptual_list = []\n",
        "min_perc = np.inf\n",
        "best_latent_code = None\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Generate the image\n",
        "    generated_image_1024 = generator(latent_code)\n",
        "\n",
        "    # Resize for FaceNet (160x160)\n",
        "    generated_image_160 = F.interpolate(generated_image_1024, size=(160, 160), mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Get Embedding\n",
        "    current_embedding = model(generated_image_160)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss_mse = mse_loss(current_embedding, target_embedding)\n",
        "\n",
        "    # Optional:\n",
        "    # Penalize if the code gets too far from the average face (prevents \"weird\" artifacts)\n",
        "    loss_reg = torch.mean((latent_code - w_avg) ** 2)\n",
        "\n",
        "    # Total Loss\n",
        "    total_loss = loss_mse + (reg_loss_weight * loss_reg)\n",
        "    # if i < 10:\n",
        "    #     print(f\"loss_mse: {loss_mse:.6f}, weighted loss_reg: {(reg_loss_weight * loss_reg):.6f}\")\n",
        "\n",
        "    perceptual_input = (generated_image_160 * 0.5) + 0.5\n",
        "    perceptual_with_real = perceptual_criterion(perceptual_input, target_image)\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{iterations}], Loss: {total_loss.item():.6f}, Perc: {perceptual_with_real.item():.6f}\")\n",
        "        # Optional: Display image periodically\n",
        "        # We take the 1024 version to see the full quality result\n",
        "        # viz_img = (generated_image_1024.detach().clamp(-1, 1) + 1) / 2.0\n",
        "        # display(transforms.ToPILImage()(viz_img[0].cpu()))\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "    perceptual_list.append(perceptual_with_real.item())\n",
        "\n",
        "    if perceptual_with_real.item() < min_perc:\n",
        "        min_perc = perceptual_with_real.item()\n",
        "        best_latent_code = copy.deepcopy(latent_code)\n",
        "\n",
        "final_image = generated_image_160.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)\n",
        "print(\"Inversion Complete.\")"
      ],
      "metadata": {
        "id": "8VMiAaFBSRoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    final_high_res = generator(latent_code)\n",
        "    # Convert to 0-1 for display\n",
        "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
        "\n",
        "    print(perceptual_list[-1])\n",
        "\n",
        "    # Save or Show\n",
        "    plt.imshow(final_img[0].permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "libRDanQSRlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    final_high_res = generator(best_latent_code)\n",
        "    # Convert to 0-1 for display\n",
        "    final_img = (final_high_res.clamp(-1, 1) + 1) / 2.0\n",
        "\n",
        "    print(min_perc)\n",
        "\n",
        "    # Save or Show\n",
        "    plt.imshow(final_img[0].permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sPokxDcmlOhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "tBMhUUpkYBeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=perceptual_list)"
      ],
      "metadata": {
        "id": "23vE3xDjYBeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp11_gmi.png\")"
      ],
      "metadata": {
        "id": "bTPmUrlJYBeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "Wi9NowhuYBeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001"
      ],
      "metadata": {
        "id": "IRzCDV5ySRgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}