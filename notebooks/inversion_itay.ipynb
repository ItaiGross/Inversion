{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iBHF3CcA2XvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "12akwXGaMDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_auF-N3R6jot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "PX-okYJJMII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dictionary of {filename: embedding_vector}\n",
        "with open(\"embeddings.pkl\", \"rb\") as f:\n",
        "    embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "zDcj2adn6tfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "CnJK2pNs2caF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87qt4Fn06Smf"
      },
      "outputs": [],
      "source": [
        "# access one entry\n",
        "print(len(embeddings))                # number of samples\n",
        "print(list(embeddings.keys())[:5])    # filenames\n",
        "vec = embeddings[\"00001.jpg\"]        # numpy array shape (512,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to torch tensor when used\n",
        "emb_target = torch.tensor(vec).unsqueeze(0).cuda()  # shape [1,512]\n",
        "print(emb_target.shape)"
      ],
      "metadata": {
        "id": "vbQEG86A6iRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160,160)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "yxs07iVJ6iKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001 = Image.open(\"00001.jpg\").convert(\"RGB\")\n",
        "x_00001 = transform(img_00001).unsqueeze(0).to(device)\n",
        "img_00002 = Image.open(\"00002.jpg\").convert(\"RGB\")\n",
        "x_00002 = transform(img_00002).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "QMaTV-hvE1pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_00001 = model(x_00001*2-1).detach().cpu().numpy()[0]\n",
        "emb_00002 = model(x_00002*2-1).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "HxMver7fFD0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001"
      ],
      "metadata": {
        "id": "Gqp1iBVxQCZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00002"
      ],
      "metadata": {
        "id": "09DzTpV2QHRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001.size, x_00001.shape"
      ],
      "metadata": {
        "id": "JrgAt_TXGXr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = np.random.randint(\n",
        "    0, 256,\n",
        "    size=(256, 256, 3),\n",
        "    dtype=np.uint8\n",
        ")\n",
        "random_img = Image.fromarray(random_data, 'RGB')"
      ],
      "metadata": {
        "id": "Nxv4pFgJOO42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_img"
      ],
      "metadata": {
        "id": "M4F6F9ZROc14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor = transform(random_img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jDlqOWknGc9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb = model(random_image_tensor*2-1).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "nUxUWx2VM4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb.shape, emb_00001.shape"
      ],
      "metadata": {
        "id": "4Lh4aCv_OqPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(random_image_emb, emb_00001)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "1ArKnrtaPCw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the random image has no relationship with the face embedding, the distance is close to 1"
      ],
      "metadata": {
        "id": "qMKfXPkwQp5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, emb_00002)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "FNYvEvDpPfDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the 2 face embeddings are intentionally different, their distance is closer to 2 which means closer to opposite."
      ],
      "metadata": {
        "id": "afKlTpQzQz2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General functions"
      ],
      "metadata": {
        "id": "uQBiOQqCtAOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_uniform(size=160):\n",
        "    tensor = torch.rand(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "0FmzuwM31QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_gaussian(size=160):\n",
        "    tensor = torch.randn(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MasWG80_JCY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_constant_and_gaussian(size=160, color_value=0.0):\n",
        "    tensor = torch.full((1, 3, size, size), fill_value=color_value, device=device)\n",
        "    # Add a tiny bit of noise to break symmetry (helps gradients start moving)\n",
        "    tensor = tensor + (torch.randn_like(tensor) * 0.01)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MeIucAFOwuc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_loss_graph(loss_list, log_scale=False):\n",
        "    plt.plot(loss_list)\n",
        "    if log_scale:\n",
        "        plt.yscale('log')\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss per Step\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FxbyYDMAtE7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_image(image, filename):\n",
        "    final_image = torch.tanh(image.detach().cpu().squeeze(0))\n",
        "    final_image = (final_image * 0.5) + 0.5\n",
        "\n",
        "    final_image = transforms.ToPILImage()(final_image)\n",
        "    final_image.save(filename)\n",
        "    display(final_image)"
      ],
      "metadata": {
        "id": "8p6TG1L_s-S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = torch.tensor(emb_00001, dtype=torch.float32).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "77TTEA00wTG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 1: uniform initialization"
      ],
      "metadata": {
        "id": "UOBRbHJ6Cn15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_uniform()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "pjQjf8sZC28H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "JhnaAaGgv-J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "GUINR4m5FHrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp1_uniform_init.png\")"
      ],
      "metadata": {
        "id": "fyzy1L4iE3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "sxi0invdFQxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 2: gaussian initialization"
      ],
      "metadata": {
        "id": "J3b8mjG72-XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_gaussian()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cQsr_RYa2-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "YuxlAgsS2-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "KOJ4Rz0t2-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp2_gaussian_init.png\")"
      ],
      "metadata": {
        "id": "o7_RZqoS2-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "epJLsL_U2-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 3: constant initialization and a bit of gaussian noise"
      ],
      "metadata": {
        "id": "xjOqVMVLPBtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "q7dZ7P8Twrwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "ICDki0ZdxHBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "OAbFkhmqPRMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp3_small_gaussian_init.png\")"
      ],
      "metadata": {
        "id": "Q7V8OnaxPRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "4hpk4vjtPRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 4: Adding Total Variation to the loss function"
      ],
      "metadata": {
        "id": "7J_GKUBiIngG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tv_loss(img_tensor):\n",
        "    \"\"\"\n",
        "    Computes Total Variation Loss.\n",
        "    Expected input shape: (Batch, Channels, Height, Width)\n",
        "    \"\"\"\n",
        "    # Calculate horizontal differences (between columns)\n",
        "    # Select all columns except the last one, minus all columns except the first one\n",
        "    diff_h = img_tensor[:, :, :, :-1] - img_tensor[:, :, :, 1:]\n",
        "\n",
        "    # Calculate vertical differences (between rows)\n",
        "    # Select all rows except the last one, minus all rows except the first one\n",
        "    diff_w = img_tensor[:, :, :-1, :] - img_tensor[:, :, 1:, :]\n",
        "\n",
        "    # Sum the absolute differences\n",
        "    tv_loss = torch.sum(torch.abs(diff_h)) + torch.sum(torch.abs(diff_w))\n",
        "\n",
        "    return tv_loss"
      ],
      "metadata": {
        "id": "MF2VwMrgPy5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 4e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "K6nLcQznxs-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "lzqefkwZPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "IArMeNtOPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp4_small_gausian_init_and_tv_weight_4e-07.png\")"
      ],
      "metadata": {
        "id": "V8ypOYrPPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "1HJ-G5BnPhPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 5: gaussian initialization with tv loss"
      ],
      "metadata": {
        "id": "4otdrts8_ukC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 1e-6\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "ydibWYAa_ukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "sx0t2lLW_ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "U1yR0Qdl_ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp5_gaussian_init_and_tv_weight_1e-06.png\")"
      ],
      "metadata": {
        "id": "L5ftr1it_ukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "GnScj21Z_ukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 6: man face initialization"
      ],
      "metadata": {
        "id": "e9Y65RT9AMEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil = Image.open(\"00000.jpg\").convert(\"RGB\")\n",
        "image = transform(image_pil).unsqueeze(0).to(device).requires_grad_(True)"
      ],
      "metadata": {
        "id": "zUbcrKdvBZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil"
      ],
      "metadata": {
        "id": "ibund_shEQpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = torch.tensor(emb_00002, dtype=torch.float32).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "jlktrplmIz5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "tv_weight = 1e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "llkZn4vrAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "Uu3Ue51TAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "hXq7oe_6AMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp6_man_face_init_and_tv_weight_1e-07_b.png\")"
      ],
      "metadata": {
        "id": "VNnltmlLAMED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00002, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "xoL-r1DrAMED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thLyDDVPsyuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 7 - Input Augmentation"
      ],
      "metadata": {
        "id": "clEqFvUMB3pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# Define a jitter transform\n",
        "jitter = T.Compose([\n",
        "    T.RandomAffine(degrees=0, translate=(0.05, 0.05))\n",
        "])\n",
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "    jittered_image = jitter(image)\n",
        "    normalized_image_input = torch.tanh(jittered_image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "7SmBWH2LB8w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp7_man_face_init_and_tv_weight_1e-07_b.png\")"
      ],
      "metadata": {
        "id": "qWFfhDB1FMBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00002, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "0I-hb_TkFc1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt 8 - cosine similarity loss"
      ],
      "metadata": {
        "id": "FiKpNWyEG2Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# Define a jitter transform\n",
        "jitter = T.Compose([\n",
        "    T.RandomAffine(degrees=0, translate=(0.05, 0.05)) # Shift image slightly\n",
        "])\n",
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    cosine_loss = 1 - torch.nn.functional.cosine_similarity(current_embedding, target_embedding, dim=1).mean()\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = cosine_loss + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "QaoCJD4lGbPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp8_man_face_init_and_tv_weight_1e-07_b_and_cosine_loss.png\")"
      ],
      "metadata": {
        "id": "2j5oBqA_HGwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X70sDl2oHbSL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}