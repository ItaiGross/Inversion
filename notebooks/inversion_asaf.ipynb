{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iBHF3CcA2XvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "12akwXGaMDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_auF-N3R6jot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "PX-okYJJMII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dictionary of {filename: embedding_vector}\n",
        "with open(\"embeddings.pkl\", \"rb\") as f:\n",
        "    embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "zDcj2adn6tfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "CnJK2pNs2caF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87qt4Fn06Smf"
      },
      "outputs": [],
      "source": [
        "# access one entry\n",
        "print(len(embeddings))                # number of samples\n",
        "print(list(embeddings.keys())[:5])    # filenames\n",
        "vec = embeddings[\"00001.jpg\"]        # numpy array shape (512,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to torch tensor when used\n",
        "emb_target = torch.tensor(vec).unsqueeze(0).cuda()  # shape [1,512]\n",
        "print(emb_target.shape)"
      ],
      "metadata": {
        "id": "vbQEG86A6iRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160,160)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "yxs07iVJ6iKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001 = Image.open(\"00001.jpg\").convert(\"RGB\")\n",
        "x_00001 = transform(img_00001).unsqueeze(0).to(device)\n",
        "img_00002 = Image.open(\"00002.jpg\").convert(\"RGB\")\n",
        "x_00002 = transform(img_00002).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "QMaTV-hvE1pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_00001 = model(x_00001*2-1).detach().cpu().numpy()[0]\n",
        "emb_00002 = model(x_00002*2-1).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "HxMver7fFD0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001"
      ],
      "metadata": {
        "id": "Gqp1iBVxQCZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00002"
      ],
      "metadata": {
        "id": "09DzTpV2QHRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001.size, x_00001.shape"
      ],
      "metadata": {
        "id": "JrgAt_TXGXr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = np.random.randint(\n",
        "    0, 256,\n",
        "    size=(256, 256, 3),\n",
        "    dtype=np.uint8\n",
        ")\n",
        "random_img = Image.fromarray(random_data, 'RGB')"
      ],
      "metadata": {
        "id": "Nxv4pFgJOO42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_img"
      ],
      "metadata": {
        "id": "M4F6F9ZROc14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor = transform(random_img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jDlqOWknGc9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb = model(random_image_tensor*2-1).detach().cpu().numpy()[0]"
      ],
      "metadata": {
        "id": "nUxUWx2VM4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb.shape, emb_00001.shape"
      ],
      "metadata": {
        "id": "4Lh4aCv_OqPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(random_image_emb, emb_00001)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "1ArKnrtaPCw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the random image has no relationship with the face embedding, the distance is close to 1"
      ],
      "metadata": {
        "id": "qMKfXPkwQp5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, emb_00002)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "FNYvEvDpPfDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the 2 face embeddings are intentionally different, their distance is closer to 2 which means closer to opposite."
      ],
      "metadata": {
        "id": "afKlTpQzQz2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General functions"
      ],
      "metadata": {
        "id": "uQBiOQqCtAOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_uniform(size=160):\n",
        "    tensor = torch.rand(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "0FmzuwM31QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_gaussian(size=160):\n",
        "    tensor = torch.randn(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MasWG80_JCY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_constant_and_gaussian(size=160, color_value=0.0):\n",
        "    tensor = torch.full((1, 3, size, size), fill_value=color_value, device=device)\n",
        "    # Add a tiny bit of noise to break symmetry (helps gradients start moving)\n",
        "    tensor = tensor + (torch.randn_like(tensor) * 0.01)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MeIucAFOwuc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_loss_graph(loss_list, log_scale=False):\n",
        "    plt.plot(loss_list)\n",
        "    if log_scale:\n",
        "        plt.yscale('log')\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss per Step\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FxbyYDMAtE7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_image(image, filename):\n",
        "    final_image = torch.tanh(image.detach().cpu().squeeze(0))\n",
        "    final_image = (final_image * 0.5) + 0.5\n",
        "\n",
        "    final_image = transforms.ToPILImage()(final_image)\n",
        "    final_image.save(filename)\n",
        "    display(final_image)"
      ],
      "metadata": {
        "id": "8p6TG1L_s-S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = torch.tensor(emb_00001, dtype=torch.float32).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "77TTEA00wTG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 1: uniform initialization"
      ],
      "metadata": {
        "id": "UOBRbHJ6Cn15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_uniform()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "pjQjf8sZC28H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "JhnaAaGgv-J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "GUINR4m5FHrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp1_uniform_init.png\")"
      ],
      "metadata": {
        "id": "fyzy1L4iE3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "sxi0invdFQxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 2: gaussian initialization"
      ],
      "metadata": {
        "id": "J3b8mjG72-XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_gaussian()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cQsr_RYa2-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "YuxlAgsS2-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "KOJ4Rz0t2-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp2_gaussian_init.png\")"
      ],
      "metadata": {
        "id": "o7_RZqoS2-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "epJLsL_U2-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 3: constant initialization and a bit of gaussian noise"
      ],
      "metadata": {
        "id": "xjOqVMVLPBtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "q7dZ7P8Twrwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "for i in tqdm(range(num_steps)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "ICDki0ZdxHBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "OAbFkhmqPRMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp3_small_gaussian_init.png\")"
      ],
      "metadata": {
        "id": "Q7V8OnaxPRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "4hpk4vjtPRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 4: Adding Total Variation to the loss function"
      ],
      "metadata": {
        "id": "7J_GKUBiIngG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tv_loss(img_tensor):\n",
        "    \"\"\"\n",
        "    Computes Total Variation Loss.\n",
        "    Expected input shape: (Batch, Channels, Height, Width)\n",
        "    \"\"\"\n",
        "    # Calculate horizontal differences (between columns)\n",
        "    # Select all columns except the last one, minus all columns except the first one\n",
        "    diff_h = img_tensor[:, :, :, :-1] - img_tensor[:, :, :, 1:]\n",
        "\n",
        "    # Calculate vertical differences (between rows)\n",
        "    # Select all rows except the last one, minus all rows except the first one\n",
        "    diff_w = img_tensor[:, :, :-1, :] - img_tensor[:, :, 1:, :]\n",
        "\n",
        "    # Sum the absolute differences\n",
        "    tv_loss = torch.sum(torch.abs(diff_h)) + torch.sum(torch.abs(diff_w))\n",
        "\n",
        "    return tv_loss"
      ],
      "metadata": {
        "id": "MF2VwMrgPy5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 4e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "K6nLcQznxs-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "lzqefkwZPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "IArMeNtOPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp4_small_gausian_init_and_tv_weight_4e-07.png\")"
      ],
      "metadata": {
        "id": "V8ypOYrPPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "1HJ-G5BnPhPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 5: gaussian initialization with tv loss"
      ],
      "metadata": {
        "id": "4otdrts8_ukC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 1e-6\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "ydibWYAa_ukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "sx0t2lLW_ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "U1yR0Qdl_ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp5_gaussian_init_and_tv_weight_1e-06.png\")"
      ],
      "metadata": {
        "id": "L5ftr1it_ukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "GnScj21Z_ukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 6: man face initialization"
      ],
      "metadata": {
        "id": "e9Y65RT9AMEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "image = transform(image_pil).unsqueeze(0).to(device).requires_grad_(True)"
      ],
      "metadata": {
        "id": "zUbcrKdvBZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil"
      ],
      "metadata": {
        "id": "ibund_shEQpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = torch.tensor(emb_00001, dtype=torch.float32).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "jlktrplmIz5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 1000\n",
        "tv_weight = 1e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "llkZn4vrAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    current_embedding = model(normalized_image_input)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "Uu3Ue51TAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "hXq7oe_6AMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp6_man_face_init_and_tv_weight_1e-07_a.png\")"
      ],
      "metadata": {
        "id": "VNnltmlLAMED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "xoL-r1DrAMED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 7: adding image jittering"
      ],
      "metadata": {
        "id": "6J3xSXXDXvL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomJitter(nn.Module):\n",
        "    def __init__(self, lim=30):\n",
        "        super(RandomJitter, self).__init__()\n",
        "        self.lim = lim\n",
        "\n",
        "    def forward(self, img_tensor):\n",
        "        B, C, H, W = img_tensor.shape\n",
        "        padded = nn.functional.pad(img_tensor, (self.lim, self.lim, self.lim, self.lim), mode='reflect')\n",
        "\n",
        "        sx = torch.randint(0, 2 * self.lim, (1,)).item()\n",
        "        sy = torch.randint(0, 2 * self.lim, (1,)).item()\n",
        "\n",
        "        jittered = padded[:, :, sy:sy+H, sx:sx+W]\n",
        "\n",
        "        return jittered"
      ],
      "metadata": {
        "id": "I1DkDj90X4Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "AsFcKUbbXvMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    jittered_image = jitter(normalized_image_input)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "94xpaDkhXvMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "rSGsWw5RXvMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp7_tv_weight_1e-07_and_jittering.png\")"
      ],
      "metadata": {
        "id": "kL48EGxMXvMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "3BhneHylXvMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 8: Decaying tv weight"
      ],
      "metadata": {
        "id": "hmngXyeYZ3rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight_per_step = [1e-6] * 250 + [1e-7] * 250 + [1e-8] * 250 + [0] * 250\n",
        "jitter = RandomJitter(lim=30)\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "49Rt0st0Z3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    jittered_image = jitter(normalized_image_input)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    tv_weight = tv_weight_per_step[i]\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "sHMPw5qWZ3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "5xdFo-EDZ3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp8_decaying_tv_weight_1e-06_to_0_and_jittering.png\")"
      ],
      "metadata": {
        "id": "oK0vhV1EZ3rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "hMhjb-UQZ3rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 9: Adding Perceptual Loss"
      ],
      "metadata": {
        "id": "q2J0wDjbzGNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "\n",
        "        # Load VGG16\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Slicing up to layer 16 (ReLU3_3) is standard.\n",
        "        self.blocks = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
        "\n",
        "        # Freeze the model weights\n",
        "        for param in self.blocks.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # VGG specific normalization\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
        "        self.resize = resize\n",
        "\n",
        "    def forward(self, generated_img, target_img):\n",
        "        # Assuming the images are in [0, 1] range:\n",
        "        gen_norm = (generated_img - self.mean) / self.std\n",
        "        target_norm = (target_img - self.mean) / self.std\n",
        "\n",
        "        # Extract features\n",
        "        gen_features = self.blocks(gen_norm)\n",
        "        target_features = self.blocks(target_norm)\n",
        "\n",
        "        # Calculate L2 loss between the feature maps\n",
        "        loss = torch.nn.functional.mse_loss(gen_features, target_features)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "OYSTOyq708Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man_face_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "man_face = transform(image_pil).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "DBEdaKfk4gsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "perceptual_criterion = VGGPerceptualLoss().to(device)\n",
        "perceptual_target_image = man_face\n",
        "perceptual_weight = 1e-3\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "EsNJb51VzGNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "perceptual_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    jittered_image = jitter(normalized_image_input)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss_perceptual = perceptual_criterion(image, perceptual_target_image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv) + (perceptual_weight * loss_perceptual)\n",
        "\n",
        "    perceptual_with_real = perceptual_criterion(image, x_00001)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}, Perc: {perceptual_with_real.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "    perceptual_list.append(perceptual_with_real.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "DAJJDY_WzGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "zBHD-_9TzGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=perceptual_list)"
      ],
      "metadata": {
        "id": "FhlgWiWu6HtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp9_tv_weight_4e-07_and_jittering_and_perceptual_with_man_face.png\")"
      ],
      "metadata": {
        "id": "KkyspNA6zGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "9Vxb7h40zGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attempt 10: Adding Perceptual Style Loss"
      ],
      "metadata": {
        "id": "60WVsiny8feS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGStyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGStyleLoss, self).__init__()\n",
        "        # Load VGG\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # List of sub-models ending at different depths\n",
        "        self.slice1 = nn.Sequential(*list(vgg.children())[:2])   # Low level (colors)\n",
        "        self.slice2 = nn.Sequential(*list(vgg.children())[:7])   # Textures\n",
        "        self.slice3 = nn.Sequential(*list(vgg.children())[:12])  # Shapes\n",
        "        self.slice4 = nn.Sequential(*list(vgg.children())[:21])  # Deep features\n",
        "\n",
        "        # Freeze model\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
        "\n",
        "    @ staticmethod\n",
        "    def gram_matrix(input_tensor):\n",
        "        batch, channels, height, width = input_tensor.size()\n",
        "\n",
        "        # Reshape so we can multiply features\n",
        "        features = input_tensor.view(batch * channels, height * width)\n",
        "\n",
        "        # Calculate the dot product\n",
        "        G = torch.mm(features, features.t())\n",
        "\n",
        "        # Normalize by the number of elements to keep values small\n",
        "        return G.div(batch * channels * height * width)\n",
        "\n",
        "    def forward(self, generated_img, guide_img):\n",
        "        gen = (generated_img - self.mean) / self.std\n",
        "        guide = (guide_img - self.mean) / self.std\n",
        "\n",
        "        loss = 0\n",
        "        # Pass through each slice\n",
        "        for slice_net in [self.slice1, self.slice2, self.slice3, self.slice4]:\n",
        "            gen_feat = slice_net(gen)\n",
        "            guide_feat = slice_net(guide)\n",
        "\n",
        "            # Compare Gram Matrices\n",
        "            gen_gram = VGGStyleLoss.gram_matrix(gen_feat)\n",
        "            guide_gram = VGGStyleLoss.gram_matrix(guide_feat)\n",
        "\n",
        "            loss += torch.nn.functional.mse_loss(gen_gram, guide_gram)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "oI3sFfv_8odd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man_face_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "man_face = transform(image_pil).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "k6xtQYhw8feU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00002"
      ],
      "metadata": {
        "id": "ip7une2vEoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = x_00002.requires_grad_(True)#get_initial_image_tensor_constant_and_gaussian()\n",
        "num_steps = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "perceptual_criterion = VGGPerceptualLoss().to(device)\n",
        "style_criterion = VGGStyleLoss().to(device)\n",
        "perceptual_target_image = man_face\n",
        "perceptual_weight = 1e2\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "5UmDPRqc8feU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "perceptual_list = []\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image_input = torch.tanh(image * 2 - 1)\n",
        "    jittered_image = normalized_image_input#jitter(normalized_image_input)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss_style = style_criterion(image, perceptual_target_image)\n",
        "    total_loss = loss_mse + (tv_weight * loss_tv) + (perceptual_weight * loss_style)\n",
        "    if i == 0:\n",
        "        # print the components of total_loss in one line, including the components' weights:\n",
        "        print(\n",
        "\n",
        "    perceptual_with_real = perceptual_criterion(image, x_00001)\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Step [{i+1}/{num_steps}], Loss: {total_loss.item():.6f}, Perc: {perceptual_with_real.item():.6f}\")\n",
        "\n",
        "    loss_list.append(total_loss.item())\n",
        "    perceptual_list.append(perceptual_with_real.item())\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "4N4DcOAN8feV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=loss_list)"
      ],
      "metadata": {
        "id": "pgosDJUJ8feV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_loss_graph(loss_list=perceptual_list)"
      ],
      "metadata": {
        "id": "-w2exgaw8feV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, \"exp10_tv_weight_4e-07_and_jittering_and_style_with_man_face.png\")"
      ],
      "metadata": {
        "id": "9GdJl-Q48feW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_dist = cosine(emb_00001, final_embedding)\n",
        "cosine_dist"
      ],
      "metadata": {
        "id": "aEVOyj0H8feW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}