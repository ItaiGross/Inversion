{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and Imports"
      ],
      "metadata": {
        "id": "iBHF3CcA2XvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch lpips torchmetrics --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "12akwXGaMDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
        "import lpips"
      ],
      "metadata": {
        "id": "_auF-N3R6jot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "PX-okYJJMII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160,160)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "ICFPu3ZOTeJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "CnJK2pNs2caF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87qt4Fn06Smf"
      },
      "outputs": [],
      "source": [
        "# load dictionary of {filename: embedding_vector}\n",
        "with open(\"embeddings.pkl\", \"rb\") as f:\n",
        "    embeddings = pickle.load(f)\n",
        "\n",
        "# access one entry\n",
        "print(f\"Number of samples: {len(embeddings)}\")\n",
        "print(f\"File names: {list(embeddings.keys())[:5]}\")\n",
        "vec = embeddings[\"00001.jpg\"]\n",
        "emb_target = torch.tensor(vec).unsqueeze(0).to(device)  # shape [1,512]\n",
        "print(f\"embedding shape: {emb_target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001 = Image.open(\"00001.jpg\").convert(\"RGB\")\n",
        "x_00001 = transform(img_00001).unsqueeze(0).to(device)\n",
        "img_00002 = Image.open(\"00002.jpg\").convert(\"RGB\")\n",
        "x_00002 = transform(img_00002).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "QMaTV-hvE1pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001"
      ],
      "metadata": {
        "id": "Gqp1iBVxQCZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00002"
      ],
      "metadata": {
        "id": "09DzTpV2QHRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00001.size, x_00001.shape"
      ],
      "metadata": {
        "id": "JrgAt_TXGXr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_00001 = model(x_00001*2-1).detach().cpu()\n",
        "emb_00002 = model(x_00002*2-1).detach().cpu()"
      ],
      "metadata": {
        "id": "HxMver7fFD0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = np.random.randint(\n",
        "    0, 256,\n",
        "    size=(256, 256, 3),\n",
        "    dtype=np.uint8\n",
        ")\n",
        "random_img = Image.fromarray(random_data, 'RGB')\n",
        "random_img"
      ],
      "metadata": {
        "id": "Nxv4pFgJOO42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_tensor = transform(random_img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jDlqOWknGc9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb = model(random_image_tensor*2-1).detach().cpu()"
      ],
      "metadata": {
        "id": "nUxUWx2VM4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_emb.shape, emb_00001.shape"
      ],
      "metadata": {
        "id": "4Lh4aCv_OqPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim = nn.functional.cosine_similarity(random_image_emb, emb_00001).item()\n",
        "cos_sim"
      ],
      "metadata": {
        "id": "U0XSquCKxqhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the random image has no relationship with the face embedding, the cosine similarity is close to 0"
      ],
      "metadata": {
        "id": "qMKfXPkwQp5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cos_sim = nn.functional.cosine_similarity(emb_00001, emb_00002).item()\n",
        "cos_sim"
      ],
      "metadata": {
        "id": "FNYvEvDpPfDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the 2 face embeddings are intentionally different, their cosine similarity is closer to -1 which means closer to opposite."
      ],
      "metadata": {
        "id": "afKlTpQzQz2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Functions"
      ],
      "metadata": {
        "id": "uQBiOQqCtAOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization Functions"
      ],
      "metadata": {
        "id": "Pdn4NYAjU1OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_uniform(size=160):\n",
        "    tensor = torch.rand(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "0FmzuwM31QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_gaussian(size=160):\n",
        "    tensor = torch.randn(1, 3, size, size, device=device)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MasWG80_JCY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_initial_image_tensor_constant_and_gaussian(size=160, color_value=0.0):\n",
        "    tensor = torch.full((1, 3, size, size), fill_value=color_value, device=device)\n",
        "    # Add a tiny bit of noise to break symmetry (helps gradients start moving)\n",
        "    tensor = tensor + (torch.randn_like(tensor) * 0.01)\n",
        "    tensor.requires_grad_(True)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "MeIucAFOwuc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Functions"
      ],
      "metadata": {
        "id": "Umlfi7XIU9NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_grid_graphs(metrics_dict, n_cols=2, steps_log=None, log_scale_keys=None, figsize=None):\n",
        "    \"\"\"\n",
        "    Plots multiple graphs in a grid.\n",
        "\n",
        "    Args:\n",
        "        metrics_dict (dict): Dictionary where Key is the Title and Value is the list of data.\n",
        "        n_cols (int): Number of columns in the grid.\n",
        "        steps_log (list): list of step jumps. If None, include all the steps.\n",
        "        log_scale_keys (list): List of keys from metrics_dict that should be plotted in log scale.\n",
        "        figsize (tuple): Optional custom size (width, height). If None, calculates automatically.\n",
        "    \"\"\"\n",
        "    if steps_log is None:\n",
        "        steps_log = list(range(len(next(iter(metrics_dict.values())))))\n",
        "\n",
        "    if log_scale_keys is None:\n",
        "        log_scale_keys = []\n",
        "\n",
        "    # Calculate Grid Dimensions\n",
        "    n = len(metrics_dict)\n",
        "    n_rows = math.ceil(n / n_cols)\n",
        "\n",
        "    # Auto-calculate figure size if not provided\n",
        "    if figsize is None:\n",
        "        figsize = (4 * n_cols, 3 * n_rows)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    # Plot Data\n",
        "    for i, (label, values) in enumerate(metrics_dict.items()):\n",
        "        ax = axes[i]\n",
        "        ax.plot(steps_log, values)\n",
        "\n",
        "        ax.set_title(f\"{label} per Step\")\n",
        "        ax.set_xlabel(\"Step\")\n",
        "        ax.set_ylabel(label)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        if label in log_scale_keys:\n",
        "            ax.set_yscale('log')\n",
        "\n",
        "    # Hide empty subplots (if n is not a perfect multiple of n_cols)\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EqnWVpgF9czI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_display_image(image, filename):\n",
        "    final_image = torch.tanh(image.detach().cpu().squeeze(0))\n",
        "    final_image = (final_image * 0.5) + 0.5\n",
        "\n",
        "    final_image = transforms.ToPILImage()(final_image)\n",
        "    final_image.save(filename)\n",
        "    display(final_image)"
      ],
      "metadata": {
        "id": "8p6TG1L_s-S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Functions"
      ],
      "metadata": {
        "id": "4REFa6SIVwR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lpips_metric = lpips.LPIPS(net='vgg')\n",
        "# < 0.25 high similarity\n",
        "# > 0.7 different images\n",
        "\n",
        "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
        "# > 30 dB: High quality (hard to distinguish difference).\n",
        "# 20-30 dB: Acceptable quality.\n",
        "# < 20 dB: Poor quality (very noisy).\n",
        "\n",
        "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
        "# 1.0: Identical images.\n",
        "# > 0.9: Very structurally similar."
      ],
      "metadata": {
        "id": "YSCsdRNjGgGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_log(i, iterations, current_img, target_img, history, freq=20):\n",
        "    \"\"\"\n",
        "    Evaluates metrics and updates history lists in-place.\n",
        "\n",
        "    Args:\n",
        "        i (int): Current iteration.\n",
        "        iterations (int): Total iterations.\n",
        "        current_img (Tensor): The normalized image (output of tanh, [-1, 1]).\n",
        "        target_img (Tensor): The target image ([0, 1]).\n",
        "        history (tuple): (lpips_list, psnr_list, ssim_list, steps).\n",
        "        freq (int): Log frequency.\n",
        "    \"\"\"\n",
        "    if i % freq != 0 and i != iterations - 1:\n",
        "        return\n",
        "\n",
        "    lpips_list, psnr_list, ssim_list, steps = history\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Convert [-1, 1] -> [0, 1]\n",
        "        val_img = (current_img * 0.5) + 0.5\n",
        "        tgt_img = target_img\n",
        "\n",
        "        # Clamp to ensure numerical stability (fix float errors like -0.0001 or 1.0001)\n",
        "        val_img = val_img.clamp(0, 1)\n",
        "        tgt_img = tgt_img.clamp(0, 1)\n",
        "\n",
        "        lpips_list.append(get_lpips_dist(val_img, tgt_img).item())\n",
        "        psnr_list.append(psnr_metric(val_img, tgt_img).item())\n",
        "        ssim_list.append(ssim_metric(val_img, tgt_img).item())\n",
        "        steps.append(i)"
      ],
      "metadata": {
        "id": "j24d-LJlcPnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the target image"
      ],
      "metadata": {
        "id": "24OBble_WTaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_embedding = emb_00001.to(device)\n",
        "target_image = x_00001"
      ],
      "metadata": {
        "id": "77TTEA00wTG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Attempts"
      ],
      "metadata": {
        "id": "i7RwPdkCWREG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 1: uniform initialization"
      ],
      "metadata": {
        "id": "UOBRbHJ6Cn15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp1_uniform_init\"\n",
        "image = get_initial_image_tensor_uniform()\n",
        "iterations = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "pjQjf8sZC28H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = ((normalized_image * 0.5) + 0.5).detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "JhnaAaGgv-J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "WS4m79Xs9pTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "WPBtzYSALiBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "fyzy1L4iE3C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final cosine similarity: {nn.functional.cosine_similarity(final_embedding, target_embedding).item()}\")"
      ],
      "metadata": {
        "id": "8C2T1-cy3r4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 2: gaussian initialization"
      ],
      "metadata": {
        "id": "J3b8mjG72-XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp2_gaussian_init\"\n",
        "image = get_initial_image_tensor_gaussian()\n",
        "iterations = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "cQsr_RYa2-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "YuxlAgsS2-XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "OznMznbLfLYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "upzZ-hNWfLYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "o7_RZqoS2-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 3: constant initialization and a bit of gaussian noise"
      ],
      "metadata": {
        "id": "xjOqVMVLPBtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp3_small_gaussian_init\"\n",
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 200\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "q7dZ7P8Twrwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "    loss = loss_fn(current_embedding, target_embedding)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "ICDki0ZdxHBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "yJIpaK7UfOma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "jhZT2R6KfOma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "Q7V8OnaxPRMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 4: adding Total Variation to the loss function"
      ],
      "metadata": {
        "id": "7J_GKUBiIngG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tv_loss(img_tensor):\n",
        "    \"\"\"\n",
        "    Computes Total Variation Loss.\n",
        "    Expected input shape: (Batch, Channels, Height, Width)\n",
        "    \"\"\"\n",
        "    # Calculate horizontal differences (between columns)\n",
        "    # Select all columns except the last one, minus all columns except the first one\n",
        "    diff_h = img_tensor[:, :, :, :-1] - img_tensor[:, :, :, 1:]\n",
        "\n",
        "    # Calculate vertical differences (between rows)\n",
        "    # Select all rows except the last one, minus all rows except the first one\n",
        "    diff_w = img_tensor[:, :, :-1, :] - img_tensor[:, :, 1:, :]\n",
        "\n",
        "    # Sum the absolute differences\n",
        "    tv_loss = torch.sum(torch.abs(diff_h)) + torch.sum(torch.abs(diff_w))\n",
        "\n",
        "    return tv_loss"
      ],
      "metadata": {
        "id": "MF2VwMrgPy5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp4_small_gausian_init_and_tv_weight_4e-07\"\n",
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight = 4e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "K6nLcQznxs-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "lzqefkwZPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "FsXtTXylfR9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "oQLuHUuCfR9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "V8ypOYrPPhPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 5: gaussian initialization with tv loss"
      ],
      "metadata": {
        "id": "4otdrts8_ukC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp5_gaussian_init_and_tv_weight_1e-06\"\n",
        "image = get_initial_image_tensor_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight = 1e-6\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "ydibWYAa_ukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "sx0t2lLW_ukE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "0ZkNRXWAfVc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "9Prekon7fVc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "L5ftr1it_ukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 6: man face initialization"
      ],
      "metadata": {
        "id": "e9Y65RT9AMEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "image = transform(image_pil).unsqueeze(0).to(device).requires_grad_(True)"
      ],
      "metadata": {
        "id": "zUbcrKdvBZxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_pil"
      ],
      "metadata": {
        "id": "ibund_shEQpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp6_man_face_init_and_tv_weight_1e-07_a\"\n",
        "iterations = 1000\n",
        "tv_weight = 1e-7\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "llkZn4vrAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "Uu3Ue51TAMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "QmaLKhtpffIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "LhmAt3DkffIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "VNnltmlLAMED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 7: adding image jittering"
      ],
      "metadata": {
        "id": "6J3xSXXDXvL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomJitter(nn.Module):\n",
        "    def __init__(self, lim=30):\n",
        "        super(RandomJitter, self).__init__()\n",
        "        self.lim = lim\n",
        "\n",
        "    def forward(self, img_tensor):\n",
        "        B, C, H, W = img_tensor.shape\n",
        "        padded = nn.functional.pad(img_tensor, (self.lim, self.lim, self.lim, self.lim), mode='reflect')\n",
        "\n",
        "        sx = torch.randint(0, 2 * self.lim, (1,)).item()\n",
        "        sy = torch.randint(0, 2 * self.lim, (1,)).item()\n",
        "\n",
        "        jittered = padded[:, :, sy:sy+H, sx:sx+W]\n",
        "\n",
        "        return jittered"
      ],
      "metadata": {
        "id": "I1DkDj90X4Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp7_tv_weight_1e-07_and_jittering\"\n",
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "AsFcKUbbXvMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    jittered_image = jitter(normalized_image)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "94xpaDkhXvMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "j2JZ0R_KfiUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "ewMAMq67fiUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "kL48EGxMXvMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 8: Decaying tv weight"
      ],
      "metadata": {
        "id": "hmngXyeYZ3rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp8_decaying_tv_weight_1e-06_to_0_and_jittering\"\n",
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight_per_step = [1e-6] * 250 + [1e-7] * 250 + [1e-8] * 250 + [0] * 250\n",
        "jitter = RandomJitter(lim=30)\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "49Rt0st0Z3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    jittered_image = jitter(normalized_image)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    tv_weight = tv_weight_per_step[i]\n",
        "    loss = loss_mse + (tv_weight * loss_tv)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "sHMPw5qWZ3rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "Lq2tAOwIflaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "Bvb58-GaflaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "oK0vhV1EZ3rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 9: adding perceptual loss"
      ],
      "metadata": {
        "id": "q2J0wDjbzGNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGPerceptualDist(nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualDist, self).__init__()\n",
        "\n",
        "        # Load VGG16\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # Slicing up to layer 16 (ReLU3_3) is standard.\n",
        "        self.blocks = nn.Sequential(*list(vgg.children())[:16]).eval()\n",
        "\n",
        "        # Freeze the model weights\n",
        "        for param in self.blocks.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # VGG specific normalization\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
        "        self.resize = resize\n",
        "\n",
        "    def forward(self, generated_img, target_img):\n",
        "        # Assuming the images are in [0, 1] range:\n",
        "        gen_norm = (generated_img - self.mean) / self.std\n",
        "        target_norm = (target_img - self.mean) / self.std\n",
        "\n",
        "        # Extract features\n",
        "        gen_features = self.blocks(gen_norm)\n",
        "        target_features = self.blocks(target_norm)\n",
        "\n",
        "        # Calculate L2 loss between the feature maps\n",
        "        loss = torch.nn.functional.mse_loss(gen_features, target_features)\n",
        "        return loss\n",
        "\n",
        "get_perc_dist = VGGPerceptualDist().to(device)"
      ],
      "metadata": {
        "id": "t9h1s9TDBM2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man_face_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "man_face = transform(image_pil).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "DBEdaKfk4gsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp9_tv_weight_4e-07_and_jittering_and_perceptual_with_man_face\"\n",
        "image = get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "perceptual_target_image = man_face\n",
        "perceptual_weight = 1e-3\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "EsNJb51VzGNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    jittered_image = jitter(normalized_image)\n",
        "    current_embedding = model(jittered_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss_perceptual = get_perc_dist(image, perceptual_target_image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv) + (perceptual_weight * loss_perceptual)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "DAJJDY_WzGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "vdvx8VYjfpPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "6fXrBnItfpPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "KkyspNA6zGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt 10: adding perceptual style loss"
      ],
      "metadata": {
        "id": "60WVsiny8feS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGStyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGStyleLoss, self).__init__()\n",
        "        # Load VGG\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # List of sub-models ending at different depths\n",
        "        self.slice1 = nn.Sequential(*list(vgg.children())[:2])   # Low level (colors)\n",
        "        self.slice2 = nn.Sequential(*list(vgg.children())[:7])   # Textures\n",
        "        self.slice3 = nn.Sequential(*list(vgg.children())[:12])  # Shapes\n",
        "        self.slice4 = nn.Sequential(*list(vgg.children())[:21])  # Deep features\n",
        "\n",
        "        # Freeze model\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(device)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(device)\n",
        "\n",
        "    @ staticmethod\n",
        "    def gram_matrix(input_tensor):\n",
        "        batch, channels, height, width = input_tensor.size()\n",
        "\n",
        "        # Reshape so we can multiply features\n",
        "        features = input_tensor.view(batch * channels, height * width)\n",
        "\n",
        "        # Calculate the dot product\n",
        "        G = torch.mm(features, features.t())\n",
        "\n",
        "        # Normalize by the number of elements to keep values small\n",
        "        return G.div(batch * channels * height * width)\n",
        "\n",
        "    def forward(self, generated_img, guide_img):\n",
        "        gen = (generated_img - self.mean) / self.std\n",
        "        guide = (guide_img - self.mean) / self.std\n",
        "\n",
        "        loss = 0\n",
        "        # Pass through each slice\n",
        "        for slice_net in [self.slice1, self.slice2, self.slice3, self.slice4]:\n",
        "            gen_feat = slice_net(gen)\n",
        "            guide_feat = slice_net(guide)\n",
        "\n",
        "            # Compare Gram Matrices\n",
        "            gen_gram = VGGStyleLoss.gram_matrix(gen_feat)\n",
        "            guide_gram = VGGStyleLoss.gram_matrix(guide_feat)\n",
        "\n",
        "            loss += torch.nn.functional.mse_loss(gen_gram, guide_gram)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "oI3sFfv_8odd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man_face_pil = Image.open(\"man_face.png\").convert(\"RGB\")\n",
        "man_face = transform(image_pil).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "k6xtQYhw8feU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_00002"
      ],
      "metadata": {
        "id": "ip7une2vEoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"exp10_tv_weight_4e-07_and_jittering_and_style_with_man_face\"\n",
        "image = x_00002.requires_grad_(True)#get_initial_image_tensor_constant_and_gaussian()\n",
        "iterations = 1000\n",
        "tv_weight = 4e-7\n",
        "jitter = RandomJitter(lim=30)\n",
        "perceptual_criterion = VGGPerceptualLoss().to(device)\n",
        "style_criterion = VGGStyleLoss().to(device)\n",
        "perceptual_target_image = man_face\n",
        "perceptual_weight = 1e2\n",
        "optimizer = optim.Adam([image], lr=0.01)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "5UmDPRqc8feU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "cosine_similarity_list = []\n",
        "\n",
        "lpips_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "steps = []\n",
        "history_lists = (lpips_list, psnr_list, ssim_list, steps)\n",
        "\n",
        "for i in tqdm(range(iterations)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    normalized_image = torch.tanh(image)\n",
        "    current_embedding = model(normalized_image)\n",
        "\n",
        "    loss_mse = loss_fn(current_embedding, target_embedding)\n",
        "    loss_tv = get_tv_loss(image)\n",
        "    loss_style = style_criterion(image, perceptual_target_image)\n",
        "    loss = loss_mse + (tv_weight * loss_tv) + (perceptual_weight * loss_style)\n",
        "    cos_sim = nn.functional.cosine_similarity(current_embedding, target_embedding).item()\n",
        "\n",
        "    evaluate_and_log(i, iterations, normalized_image, target_image, history_lists, freq=20)\n",
        "\n",
        "    if cos_sim > 0.95:\n",
        "        print(f\" Early stopping at iteration: {i}! Cosine Similarity: {cos_sim:.6f}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i == 0 or (i + 1) % int(iterations / 10) == 0:\n",
        "        print(f\" Step [{i+1}/{iterations}], Loss: {loss.item():.6f}, cos sim: {cos_sim:.6f}\")\n",
        "\n",
        "    loss_list.append(loss.item())\n",
        "    cosine_similarity_list.append(cos_sim)\n",
        "\n",
        "final_image = image.detach().cpu().squeeze(0)\n",
        "final_embedding = current_embedding.detach().cpu().squeeze(0)"
      ],
      "metadata": {
        "id": "4N4DcOAN8feV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"Loss\": loss_list,\n",
        "    \"Cosine Similarity\": cosine_similarity_list\n",
        "}, n_cols=3)"
      ],
      "metadata": {
        "id": "HqhTuOlmft5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_grid_graphs({\n",
        "    \"LPIPS\": lpips_list,\n",
        "    \"PSNR\": psnr_list,\n",
        "    \"SSIM\": ssim_list\n",
        "}, n_cols=3, steps_log=steps)"
      ],
      "metadata": {
        "id": "P-rfdjJKft5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_and_display_image(final_image, f\"{experiment_name}.png\")"
      ],
      "metadata": {
        "id": "9GdJl-Q48feW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}